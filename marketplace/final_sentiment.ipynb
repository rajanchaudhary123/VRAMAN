{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "872a720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6553954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CPN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CPN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4622ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_excel('./project_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d26e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love monaco staff husband stayed hotel crazy w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiments\n",
       "0  nice hotel expensive parking got good deal sta...   positive\n",
       "1  ok nothing special charge diamond member hilto...   negative\n",
       "2  unique, great stay, wonderful time hotel monac...   positive\n",
       "3  great stay great stay, went seahawk game aweso...   positive\n",
       "4  love monaco staff husband stayed hotel crazy w...   positive"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e506b7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Map sentiment labels to numerical values\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiments\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Map sentiment labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiments'] = label_encoder.fit_transform(data['Sentiments'])\n",
    "\n",
    "\n",
    "#num_classes = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebbe9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b5b1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf98107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the 'Text' column\n",
    "data['Review'] = data['Review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c949382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['Review'], data['Sentiments'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b9163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Tokenize the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "#Tokenization involves converting each word in the text into a unique integer ID. 'this': 1,'is': 2,  'first': 4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e4160af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the testing data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afeb1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to ensure consistent length\n",
    "max_sequence_length = 100  # Set the desired maximum sequence length\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "#The tokenized sequences are padded to ensure a consistent length for each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38310adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "num_classes = len(label_encoder.classes_)\n",
    "train_labels_categorical = to_categorical(train_labels, num_classes=num_classes)\n",
    "test_labels_categorical = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "#cat:  [1, 0, 0] dog:  [0, 1, 0] cat:  [1, 0, 0] One-hot encoding represents each label as a binary vector of the same length as the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "201ca077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "# we set it to 100, meaning each word will be represented as a 100-dimensional vector.\n",
    "model.add(LSTM(100))\n",
    "#The parameter 100 specifies the number of LSTM units or cells in the layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#adds a dense layer to the model.\n",
    "#for  two classes (positive and negative), num_classes would be set to 2.\n",
    "#The softmax function applies the exponential function and normalizes the values so that they sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4c3cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "371ebb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "458/458 [==============================] - 61s 126ms/step - loss: 0.2244 - accuracy: 0.9121 - val_loss: 0.1703 - val_accuracy: 0.9356\n",
      "Epoch 2/10\n",
      "458/458 [==============================] - 58s 127ms/step - loss: 0.0877 - accuracy: 0.9703 - val_loss: 0.2123 - val_accuracy: 0.9356\n",
      "Epoch 3/10\n",
      "458/458 [==============================] - 58s 127ms/step - loss: 0.0399 - accuracy: 0.9876 - val_loss: 0.2434 - val_accuracy: 0.9295\n",
      "Epoch 4/10\n",
      "458/458 [==============================] - 61s 132ms/step - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.3035 - val_accuracy: 0.9085\n",
      "Epoch 5/10\n",
      "458/458 [==============================] - 64s 140ms/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.3173 - val_accuracy: 0.9175\n",
      "Epoch 6/10\n",
      "458/458 [==============================] - 61s 132ms/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 0.3054 - val_accuracy: 0.9263\n",
      "Epoch 7/10\n",
      "458/458 [==============================] - 59s 129ms/step - loss: 0.0260 - accuracy: 0.9915 - val_loss: 0.3319 - val_accuracy: 0.9246\n",
      "Epoch 8/10\n",
      "458/458 [==============================] - 70s 152ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.4631 - val_accuracy: 0.9265\n",
      "Epoch 9/10\n",
      "458/458 [==============================] - 63s 138ms/step - loss: 2.1830e-04 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 0.9268\n",
      "Epoch 10/10\n",
      "458/458 [==============================] - 62s 135ms/step - loss: 5.0950e-05 - accuracy: 1.0000 - val_loss: 0.5526 - val_accuracy: 0.9263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199a6ffbb20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_sequences, train_labels_categorical, validation_data=(test_sequences, test_labels_categorical), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b925e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and label encoder\n",
    "tokenizer_path = './tokenizer.pkl'\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cea7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_path = './label_encoder.pkl'\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0308d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = 'sentiment_model.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d036434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9262697696685791\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "_, train_accuracy = model.evaluate(train_sequences, train_labels_categorical, verbose=0)\n",
    "_, test_accuracy = model.evaluate(test_sequences, test_labels_categorical, verbose=0)\n",
    "\n",
    "# Print accuracy\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ab8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
